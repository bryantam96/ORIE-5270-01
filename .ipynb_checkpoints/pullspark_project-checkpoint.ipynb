{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, os\n",
    "import pandas as pd\n",
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ({'LESS_TERMCAP_mb': '\\x1b[01;31m', 'HOSTNAME': 'ip-172-31-45-112', 'LESS_TERMCAP_md': '\\x1b[01;38;5;208m', 'LESS_TERMCAP_me': '\\x1b[0m', 'SHELL': '/bin/bash', 'TERM': 'xterm-color', 'HISTSIZE': '1000', 'SSH_CLIENT': '128.84.95.213 57316 22', 'EC2_AMITOOL_HOME': '/opt/aws/amitools/ec2', 'LESS_TERMCAP_ue': '\\x1b[0m', 'SSH_TTY': '/dev/pts/3', 'USER': 'ec2-user', 'LS_COLORS': 'rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=01;05;37;41:su=37;41:sg=30;43:ca=30;41:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.Z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.jpg=01;35:*.jpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.axv=01;35:*.anx=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=01;36:*.au=01;36:*.flac=01;36:*.mid=01;36:*.midi=01;36:*.mka=01;36:*.mp3=01;36:*.mpc=01;36:*.ogg=01;36:*.ra=01;36:*.wav=01;36:*.axa=01;36:*.oga=01;36:*.spx=01;36:*.xspf=01;36:', 'EC2_HOME': '/opt/aws/apitools/ec2', 'TERMCAP': 'SC|screen|VT 100/ANSI X3.64 virtual terminal:\\\\\\n\\t:DO=\\\\E[%dB:LE=\\\\E[%dD:RI=\\\\E[%dC:UP=\\\\E[%dA:bs:bt=\\\\E[Z:\\\\\\n\\t:cd=\\\\E[J:ce=\\\\E[K:cl=\\\\E[H\\\\E[J:cm=\\\\E[%i%d;%dH:ct=\\\\E[3g:\\\\\\n\\t:do=^J:nd=\\\\E[C:pt:rc=\\\\E8:rs=\\\\Ec:sc=\\\\E7:st=\\\\EH:up=\\\\EM:\\\\\\n\\t:le=^H:bl=^G:cr=^M:it#8:ho=\\\\E[H:nw=\\\\EE:ta=^I:is=\\\\E)0:\\\\\\n\\t:li#49:co#78:am:xn:xv:LP:sr=\\\\EM:al=\\\\E[L:AL=\\\\E[%dL:\\\\\\n\\t:cs=\\\\E[%i%d;%dr:dl=\\\\E[M:DL=\\\\E[%dM:dc=\\\\E[P:DC=\\\\E[%dP:\\\\\\n\\t:im=\\\\E[4h:ei=\\\\E[4l:mi:IC=\\\\E[%d@:ks=\\\\E[?1h\\\\E=:\\\\\\n\\t:ke=\\\\E[?1l\\\\E>:vi=\\\\E[?25l:ve=\\\\E[34h\\\\E[?25h:vs=\\\\E[34l:\\\\\\n\\t:ti=\\\\E[?1049h:te=\\\\E[?1049l:us=\\\\E[4m:ue=\\\\E[24m:so=\\\\E[3m:\\\\\\n\\t:se=\\\\E[23m:mb=\\\\E[5m:md=\\\\E[1m:mr=\\\\E[7m:me=\\\\E[m:ms:\\\\\\n\\t:Co#8:pa#64:AF=\\\\E[3%dm:AB=\\\\E[4%dm:op=\\\\E[39;49m:AX:\\\\\\n\\t:vb=\\\\Eg:G0:as=\\\\E(0:ae=\\\\E(B:\\\\\\n\\t:ac=\\\\140\\\\140aaffggjjkkllmmnnooppqqrrssttuuvvwwxxyyzz{{||}}~~..--++,,hhII00:\\\\\\n\\t:po=\\\\E[5i:pf=\\\\E[4i:k0=\\\\E[10~:k1=\\\\EOP:k2=\\\\EOQ:k3=\\\\EOR:\\\\\\n\\t:k4=\\\\EOS:k5=\\\\E[15~:k6=\\\\E[17~:k7=\\\\E[18~:k8=\\\\E[19~:\\\\\\n\\t:k9=\\\\E[20~:k;=\\\\E[21~:F1=\\\\E[23~:F2=\\\\E[24~:F3=\\\\E[1;2P:\\\\\\n\\t:F4=\\\\E[1;2Q:F5=\\\\E[1;2R:F6=\\\\E[1;2S:F7=\\\\E[15;2~:\\\\\\n\\t:F8=\\\\E[17;2~:F9=\\\\E[18;2~:FA=\\\\E[19;2~:kb=\\x7f:K2=\\\\EOE:\\\\\\n\\t:kB=\\\\E[Z:kF=\\\\E[1;2B:kR=\\\\E[1;2A:*4=\\\\E[3;2~:*7=\\\\E[1;2F:\\\\\\n\\t:#2=\\\\E[1;2H:#3=\\\\E[2;2~:#4=\\\\E[1;2D:%c=\\\\E[6;2~:%e=\\\\E[5;2~:\\\\\\n\\t:%i=\\\\E[1;2C:kh=\\\\E[1~:@1=\\\\E[1~:kH=\\\\E[4~:@7=\\\\E[4~:\\\\\\n\\t:kN=\\\\E[6~:kP=\\\\E[5~:kI=\\\\E[2~:kD=\\\\E[3~:ku=\\\\EOA:kd=\\\\EOB:\\\\\\n\\t:kr=\\\\EOC:kl=\\\\EOD:km:', 'LESS_TERMCAP_us': '\\x1b[04;38;5;111m', 'PATH': '/home/ec2-user/anaconda3/bin:/home/ec2-user/anaconda3/bin:/home/ec2-user/anaconda3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/sbin:/opt/aws/bin:/home/ec2-user/.local/bin:/home/ec2-user/bin:/opt/aws/bin', 'MAIL': '/var/spool/mail/ec2-user', 'STY': '1084.pts-3.ip-172-31-45-112', 'PWD': '/home/ec2-user', 'JAVA_HOME': '/usr/lib/jvm/jre', 'LANG': 'en_GB.UTF-8', 'AWS_CLOUDWATCH_HOME': '/opt/aws/apitools/mon', 'HISTCONTROL': 'ignoredups', 'HOME': '/home/ec2-user', 'SHLVL': '2', 'AWS_PATH': '/opt/aws', 'AWS_AUTO_SCALING_HOME': '/opt/aws/apitools/as', 'LOGNAME': 'ec2-user', 'WINDOW': '0', 'SSH_CONNECTION': '128.84.95.213 57316 172.31.45.112 22', 'AWS_ELB_HOME': '/opt/aws/apitools/elb', 'LESSOPEN': '||/usr/bin/lesspipe.sh %s', 'LESS_TERMCAP_se': '\\x1b[0m', '_': '/home/ec2-user/anaconda3/bin/jupyter', 'JPY_PARENT_PID': '1108', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ.get('SPARK_HOME', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f0ed7031e2c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# If Spark V1.4.x is detected, then add ' pyspark-shell' to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# the end of the 'PYSPARK_SUBMIT_ARGS' environment variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mspark_release_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark_home\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/RELEASE\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_release_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"Spark 1.4\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_release_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpyspark_submit_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PYSPARK_SUBMIT_ARGS\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'str'"
     ]
    }
   ],
   "source": [
    "#os.environ.get('PYSPARK_SUBMIT_ARGS', None)\n",
    "#os.environ.get(\"SPARK_HOME\", None)\n",
    "\n",
    "# Spark home\n",
    "spark_home = os.environ.get(\"SPARK_HOME\")\n",
    "\n",
    "# If Spark V1.4.x is detected, then add ' pyspark-shell' to\n",
    "# the end of the 'PYSPARK_SUBMIT_ARGS' environment variable\n",
    "spark_release_file = spark_home + \"/RELEASE\"\n",
    "if os.path.exists(spark_release_file) and \"Spark 1.4\" in open(spark_release_file).read():\n",
    "    pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "    if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "# Add the spark python sub-directory to the path\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "\n",
    "# Add the py4j to the path.\n",
    "# You may need to change the version number to match your install\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.8.2.1-src.zip\"))\n",
    "\n",
    "# Initialize PySpark to predefine the SparkContext variable 'sc'\n",
    "execfile(os.path.join(spark_home, \"python/pyspark/shell.py\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Java gateway process exited before sending the driver its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ce9032ab9ce4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetSystemProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spark.executor.memory'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'8g'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taqmaster20141201'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36msetSystemProperty\u001b[0;34m(cls, key, value)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0minvoked\u001b[0m \u001b[0mbefore\u001b[0m \u001b[0minstantiating\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \"\"\"\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSystem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mcallback_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgateway_port\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Java gateway process exited before sending the driver its port number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# In Windows, ensure the Java child processes do not linger after Python has exited.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Java gateway process exited before sending the driver its port number"
     ]
    }
   ],
   "source": [
    "SparkContext.setSystemProperty('spark.executor.memory', '8g')\n",
    "conf = SparkConf()\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "lines = sc.textFile('taqmaster20141201')\n",
    "csval = lines.map(lambda x: re.split(',', x))\n",
    "\n",
    "def splitter(elem1):\n",
    "    return elem1[0][0:5],elem1[0][5:10],elem1[1],elem1[2][0:10],elem1[2][11:22],elem1[2][22:],elem1[3]\n",
    "    \n",
    "#csvfn = csval.map(splitter)\n",
    "data1 = pd.DataFrame(csval.collect())\n",
    "#data1.columns = ['Client','Location','Type','Broker','CUSIP','Transaction','Date']\n",
    "data1 = data1.drop([0]).reset_index(drop=True)\n",
    "sc.stop()\n",
    "\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
